## Tools file:

Using Tools with LangChain (Step-by-Step)
This example shows how to give an LLM access to a Python function (tool) and let the model decide when to call it.

---

1Ô∏è‚É£ Defining a Tool
from langchain.tools import tool

@tool
def get_weather(location: str) -> str:
"""Get the weather at a location"""
return f"It's sunny in {location}"
What this does
‚Ä¢ @tool converts a normal Python function into a LangChain Tool
‚Ä¢ The docstring tells the LLM what the tool does
‚Ä¢ The type hint (location: str) tells the LLM what input it needs
Now the model understands:
üëâ There exists a tool named get_weather
üëâ It expects a location argument

---

2Ô∏è‚É£ Binding Tools to the Model
model_with_tools = model.bind_tools([get_weather])
What this does
‚Ä¢ Attaches the get_weather tool to the LLM
‚Ä¢ Allows the model to decide when to use it
Without this step, the model cannot call tools.

---

3Ô∏è‚É£ Model Generates Tool Calls
response = model_with_tools.invoke("What is the weather in India?")
print(response)
What happens
Instead of answering directly, the model returns:
‚Ä¢ A tool name
‚Ä¢ Tool arguments
Example internally:
{
"name": "get_weather",
"args": {"location": "India"}
}
This is called a tool call.

---

Viewing Tool Calls
for tool_call in response.tool_calls:
print(f"Tool: {tool_call['name']}")
print(f"Args: {tool_call['args']}")
Output
Tool: get_weather
Args: {'location': 'India'}
Meaning
The LLM decided:

- Call get_weather
- Pass location="India"
