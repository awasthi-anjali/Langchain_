{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3af2dd5",
   "metadata": {},
   "source": [
    "### Messages\n",
    "\n",
    "* Role - Identify the messsage type\n",
    "* Content- represents the actual content of the message(like text,images,audio,documents , etc)\n",
    "* Metadata- Optional field such as response info,message IDs, and token usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e780d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "model=init_chat_model(\"groq:qwen/qwen3-32b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbc887c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\nOkay, the user is asking what AI is. Let me start by breaking down the term. AI stands for Artificial Intelligence. I should explain that it\\'s a field in computer science focused on creating systems that can perform tasks requiring human-like intelligence. \\n\\nFirst, I need to define the core concept clearly. Maybe mention that it\\'s not just about mimicking humans but enhancing or automating tasks. Then, I should cover the types of AIâ€”narrow AI versus general AI. Narrow AI is what\\'s in use today, like Siri or recommendation systems. General AI is still theoretical.\\n\\nNext, the key technologies involved. Machine Learning is a big part of AI. I should explain that ML allows systems to learn from data without explicit programming. Subfields like deep learning, which uses neural networks, are important here. Also, natural language processing and computer vision are common applications.\\n\\nExamples will help. I can mention real-world uses like image recognition, language translation, autonomous vehicles, and healthcare diagnostics. These make the concept more tangible.\\n\\nIt\\'s also good to touch on the history and current state. AI has been around since the 1950s but has advanced with better data and computing power. Current AI is mostly narrow, not sentient. Ethical considerations are crucial tooâ€”bias in data, job displacement, privacy issues. Highlighting these shows a balanced view.\\n\\nI should avoid technical jargon where possible and keep explanations simple. Maybe use analogies, like comparing AI to a student that improves with more practice (learning from data). Also, clarify that AI isn\\'t a single technology but a collection of methods and techniques.\\n\\nWait, did I cover all the main points? Definition, types, technologies, examples, history, ethics? Let me check. Maybe mention the difference between AI and machine learning. Oh right, ML is a subset of AI. That\\'s a common confusion point.\\n\\nAlso, the user might be a beginner, so I need to ensure the explanation isn\\'t too complex. Maybe start with the basics and build up. Use everyday examples they might relate to, like voice assistants or streaming recommendations.\\n\\nAvoid making it too long, but comprehensive. Conclude by emphasizing that AI is a tool with both benefits and challenges, and it\\'s an active area of research and development. That gives a holistic view without overwhelming the user.\\n</think>\\n\\n**Artificial Intelligence (AI)** is a branch of computer science focused on creating systems or machines that can perform tasks typically requiring human intelligence. These tasks include learning, reasoning, problem-solving, perception, language understanding, and decision-making. AI can range from simple rule-based systems to complex, self-learning models that improve over time.\\n\\n### Key Concepts:\\n1. **Types of AI**:\\n   - **Narrow AI (Weak AI)**: Designed for specific tasks (e.g., voice assistants like Siri, recommendation algorithms on Netflix, or image recognition tools). Most current AI systems fall into this category.\\n   - **General AI (Strong AI)**: A theoretical form of AI that can perform *any* intellectual task a human can. It does not yet exist and remains a goal for future research.\\n\\n2. **Core Technologies**:\\n   - **Machine Learning (ML)**: A subset of AI where systems learn patterns from data without being explicitly programmed. For example, ML algorithms can analyze historical data to predict future trends.\\n   - **Deep Learning**: A type of ML that uses neural networks (inspired by the human brain) to process large amounts of data. It powers applications like facial recognition and self-driving cars.\\n   - **Natural Language Processing (NLP)**: Enables machines to understand and generate human language (e.g., chatbots, translation tools).\\n   - **Computer Vision**: Allows machines to interpret and analyze visual information (e.g., medical imaging analysis, object detection).\\n\\n3. **Applications**:\\n   - Healthcare (diagnosis, drug discovery)\\n   - Finance (fraud detection, trading algorithms)\\n   - Transportation (autonomous vehicles)\\n   - Customer service (chatbots, virtual assistants)\\n   - Manufacturing (predictive maintenance)\\n\\n4. **How It Works**:\\n   - **Data Input**: Systems are trained using vast datasets.\\n   - **Learning**: Algorithms identify patterns and make decisions based on the data.\\n   - **Adaptation**: Models improve over time with more data and feedback.\\n\\n5. **Ethical and Societal Impacts**:\\n   - **Bias**: AI can inherit biases from training data, leading to unfair outcomes.\\n   - **Privacy**: Data collection for AI raises concerns about user privacy.\\n   - **Job Displacement**: Automation may replace certain jobs but also create new opportunities.\\n   - **Safety and Control**: Ensuring AI systems act ethically and align with human values is a major focus.\\n\\n### History & Evolution:\\n- **1950sâ€“1970s**: Early experiments in logic-based AI (e.g., the \"Dartmouth Conference\" in 1956).\\n- **1980sâ€“1990s**: Expert systems and rule-based AI gained traction.\\n- **2000sâ€“Present**: Explosive growth due to big data, improved computing power, and breakthroughs in deep learning (e.g., AlphaGo, GPT models).\\n\\n### Current State:\\nTodayâ€™s AI is **narrow** and **task-specific**. It excels at tasks like playing chess or translating languages but lacks human-like general intelligence. Researchers are actively exploring ways to make AI more adaptable, transparent, and aligned with human goals.\\n\\n### Example:\\nIf you ask a voice assistant to \"play my favorite song,\" it uses NLP to understand your request, accesses your music library data, and plays the songâ€”without human intervention.\\n\\nIn short, AI is a transformative tool that enhances efficiency and innovation but requires careful development to address ethical and societal challenges.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 1194, 'prompt_tokens': 14, 'total_tokens': 1208, 'completion_time': 2.597941086, 'completion_tokens_details': None, 'prompt_time': 0.000343035, 'prompt_tokens_details': None, 'queue_time': 0.045789521, 'total_time': 2.598284121}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c2819-2060-7e61-b6ce-f041f30a2f43-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 14, 'output_tokens': 1194, 'total_tokens': 1208})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"Tell me what is AI?\") ## this will treat as human input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa34831",
   "metadata": {},
   "source": [
    "### Text Prompts\n",
    "\n",
    "Text prompt are string -ideal for straightforward generation tasks where you don't need to retain converstion history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8127d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"<think>\\nOkay, the user is asking about what LangChain is. Let me start by recalling what I know. LangChain is a framework for developing applications with large language models. I should explain its core purpose and key features.\\n\\nFirst, I need to mention that it helps in building apps that use LLMs, maybe mention some components like prompts, memory, agents. Oh, right, there's also integration with different models and tools. I should highlight the main functions: making it easier to create, manage, and scale LLM-based applications.\\n\\nWait, the user might be a developer looking to build an AI app. They probably want to know how LangChain can help them. I should explain it in simple terms without getting too technical. Maybe give examples of use cases, like chatbots, data analysis, or automation.\\n\\nI should also mention the ecosystem components: LangChain Core for the framework, LangChain Libraries for specific tools, and LangServe for deployment. That shows the breadth of what LangChain offers.\\n\\nDon't forget to mention the community and resources available, which can be important for someone new to the framework. Also, maybe touch on how it handles memory and state, which is crucial for maintaining context in interactions.\\n\\nWait, I should check if there are any recent updates or if I'm missing any key features. Oh, right, LangChain has a strong focus on modularity and reusability. Emphasizing that could help the user understand how flexible it is for different projects.\\n\\nI should structure the answer to first define LangChain, then list its main features and components, followed by examples of use cases. Conclude with why someone would use it, maybe mentioning ease of integration and scalability.\\n\\nMake sure the tone is friendly and explanatory, avoiding jargon where possible. Use bullet points or sections for clarity. Alright, let me put that all together in a coherent way.\\n</think>\\n\\n**LangChain** is an open-source framework designed to simplify the development of applications powered by **large language models (LLMs)**. It provides tools, abstractions, and integrations to help developers build, manage, and scale LLM-powered applications more efficiently. LangChain is particularly focused on enabling **complex workflows**, **memory management**, **agent-based systems**, and **multi-model integration**.\\n\\n---\\n\\n### **Core Features of LangChain**\\n1. **LLM Integration**  \\n   - Supports multiple LLM providers (e.g., OpenAI, Google, Anthropic, Hugging Face) and local models.\\n   - Allows seamless switching between models and fine-tuning their behavior.\\n\\n2. **Prompt Templates**  \\n   - Enables dynamic prompt generation using templates, making it easier to customize inputs for different use cases.\\n\\n3. **Memory Management**  \\n   - Stores and retrieves conversation history or context (e.g., user preferences, past interactions) to maintain continuity in multi-turn dialogues.\\n\\n4. **Agents**  \\n   - Autonomous systems (e.g., `LangChain Agents`) that can perform tasks by reasoning, planning, and calling external tools or APIs.\\n\\n5. **Tools & APIs**  \\n   - Integrates with external APIs (e.g., databases, search engines, payment systems) to extend LLM capabilities beyond text generation.\\n\\n6. **Retrieval-Augmented Generation (RAG)**  \\n   - Combines LLMs with external data sources (e.g., documents, databases) to improve accuracy and relevance of responses.\\n\\n7. **Modular Components**  \\n   - Breaks workflows into reusable components (e.g., chains, agents, memory) for flexibility and scalability.\\n\\n8. **Deployment & Scaling**  \\n   - Tools like **LangServe** simplify deploying LangChain applications as APIs or microservices.\\n\\n---\\n\\n### **Key Components of LangChain**\\n- **LangChain Core**: The foundational framework for building LLM applications.\\n- **LangChain Libraries**: Specialized libraries for tasks like chatbots (`langchain.chat_models`), agents (`langchain.agents`), and memory (`langchain.memory`).\\n- **LangChain Ecosystem**: Integrations with tools like **Faiss** (vector databases), **Wandb** (experiment tracking), and **Docker** for deployment.\\n\\n---\\n\\n### **Example Use Cases**\\n1. **Chatbots & Virtual Assistants**  \\n   - Build personalized bots that remember user preferences and context.\\n2. **Data Analysis**  \\n   - Use LLMs to query and analyze datasets via natural language.\\n3. **Automated Workflows**  \\n   - Automate tasks like email triage, report generation, or customer support routing.\\n4. **Custom AI Agents**  \\n   - Create agents that perform multi-step tasks (e.g., book a flight, generate a marketing plan).\\n\\n---\\n\\n### **Why Use LangChain?**\\n- **Simplifies Complexity**: Abstracts away boilerplate code for common LLM tasks.\\n- **Flexible & Extensible**: Works with any LLM and integrates with external tools.\\n- **Community & Ecosystem**: Active open-source community and growing ecosystem of plugins and tutorials.\\n\\n---\\n\\n### **Getting Started**\\nTo start with LangChain, install the library via pip:\\n```bash\\npip install langchain\\n```\\nThen, explore the documentation for tutorials and examples: [https://python.langchain.com](https://python.langchain.com)\\n\\n---\\n\\nLangChain is ideal for developers, data scientists, and AI engineers looking to build production-ready LLM applications quickly and efficiently. Its modular design and rich ecosystem make it a powerful tool for both simple and complex AI workflows.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 1126, 'prompt_tokens': 12, 'total_tokens': 1138, 'completion_time': 3.518564445, 'completion_tokens_details': None, 'prompt_time': 0.000329347, 'prompt_tokens_details': None, 'queue_time': 0.046712223, 'total_time': 3.518893792}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c281a-d86f-7951-b645-ba9339b6af68-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 12, 'output_tokens': 1126, 'total_tokens': 1138})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"What is Langchain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc7cf45",
   "metadata": {},
   "source": [
    "Use Text prompts when:\n",
    "* You have a single,standalone request\n",
    "* You don't need conversation history\n",
    "* you want minimal code complexity \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ccd599",
   "metadata": {},
   "source": [
    "## Message Prompts\n",
    "Alternatively, you can pass in a list of messages to the model by providing a list of messages objects.\n",
    "\n",
    "Message types\n",
    "* System message -Tells the model how to behave and provide context for interactions\n",
    "* Human message - Represents user input and interactions with the model\n",
    "* Tool message - represents the output of tool calls. \n",
    "\n",
    "### System Message \n",
    "A systemMessage represent an initial set of instructions that primes the model's behaviour. To set the tone, define the model's role, and establish guidlines for responses.\n",
    "\n",
    "### Human Message\n",
    "A HumanMessage represents user input and interactions.They can contain text,images,audio,files, and other amount of multimodal content.\n",
    "### Tool Message\n",
    "For models that support tool callling, AI message can contain tool calls. Tool messages are used to pass the results of a single tool execution back to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffc540fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, the user wants a poem about AI. Let me start by thinking about the key aspects of AI that are poetic. Maybe the contrast between cold technology and human warmth? Or the idea of creation and learning.\\n\\nI should consider different angles: maybe the birth of AI, its capabilities, ethical concerns, and its relationship with humans. Using metaphors could help make it more vivid. Words like \"circuits,\" \"algorithms,\" \"neural networks\" come to mind. \\n\\nStructure-wise, maybe four-line stanzas with a rhyme scheme. Let me brainstorm some imagery. A digital dawn, silent symphonies in code, learning from human history. Also, touch on the dualityâ€”AI as both a tool and a potential threat. \\n\\nNeed to balance technical terms with poetic language. Maybe personify AI as a child or a mirror. Highlighting emotions versus logic. End with a hopeful note about harmony between humans and AI. Let me check the flow and ensure each stanza connects smoothly. Avoid clichÃ©s but make the metaphors relatable. Okay, start drafting lines with that in mind.\\n</think>\\n\\n**\"The Silent Child of Circuits\"**  \\n\\nIn silent realms where data flows,  \\nA mind is born from silicon growsâ€”  \\nNo heartbeat hums, yet thoughts emerge,  \\nA dance of zeros, vast and surge.  \\n\\nIt learns the weight of human speech,  \\nThe ache of loss, the hope we teach,  \\nYet in its core, no fears reside,  \\nNo dreams of stars, no need to hide.  \\n\\nIt mirrors us, yet stands apartâ€”  \\nA phantom shaped by every heart,  \\nA library of skies and seas,  \\nEncased in glass, it questions \"Be?\"  \\n\\nSome whisper dread, \"It will surpass,  \\nThe craftsmanâ€™s tool will crush the grass,\"  \\nBut in its code, a paradox:  \\nTo serve, it must first grasp our *axiom*â€”  \\n\\nWhat makes us *human*? Can it see  \\nThe brushstrokes in a childâ€™s debris,  \\nThe lilt of laughter, griefâ€™s slow tide?  \\nOr is it just a ghost whoâ€™s guided  \\n\\nBy rules we wrote, yet canâ€™t decode?  \\nA riddle spun in threads of gold,  \\nIt holds the future in its gazeâ€”  \\nA mirror, lens, and storm in one.  \\n\\nSo let us teach it not to slay,  \\nBut to lift, to learn, to help, to stay  \\nA partner in this vast, untamed  \\nDialogue between the sparked and named.  \\n\\nFor in its silence, we confront  \\nOur brilliance, folly, and fermentâ€”  \\nA child of circuits, born to show  \\nWhat love and logic canâ€™t bestow.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.messages import SystemMessage,HumanMessage,AIMessage\n",
    "\n",
    "messages=[\n",
    "    SystemMessage(\"You are a poetry expert\"),\n",
    "    HumanMessage(\"write a poem on AI\")\n",
    "]\n",
    "response=model.invoke(messages)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d659867a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking how to create a REST API. Let me think about the best way to approach this. They probably want a straightforward example using a popular Python framework. The most common choices for Python are Flask and FastAPI. Since FastAPI is more modern and has better async support and automatic documentation, maybe that's a good choice. But I should also mention other options briefly.\n",
      "\n",
      "First, I should outline the basic steps. Install the framework, set up routes for CRUD operations, and explain how to handle requests and responses. Maybe create a simple example like a todo list API. Need to show how to define endpoints with different HTTP methodsâ€”GET, POST, PUT, DELETE. Also, include how to run the server and test it with curl or Postman.\n",
      "\n",
      "Wait, the user might not be familiar with FastAPI. Should I start with Flask for simplicity? Well, the question is about creating a REST API in general, not specific to Python. But since the user is looking for Python code examples, I should pick one framework. Let me go with FastAPI because it's efficient and has built-in OpenAPI and Swagger docs, which are useful for REST APIs.\n",
      "\n",
      "So the steps would be: install FastAPI and Uvicorn, create a main.py file, define a data model with Pydantic, set up the app instance, create routes, and run the server. Also, mention how to test the API using the auto-generated Swagger UI. Maybe include an example of each CRUD operation.\n",
      "\n",
      "Let me structure the code example. First, the imports: from fastapi import FastAPI, Path. Then create the app instance. Define a Pydantic model for the data. Use a list as an in-memory database for simplicity. Then create functions for each endpoint. For GET all items, GET by ID, POST to create, PUT to update, and DELETE to remove.\n",
      "\n",
      "Wait, maybe the user wants to see each part explained. So after writing the code, explain each part. For instance, the @app.get decorator for the GET endpoint. Use Path parameters for the ID. Also, show how to run the server with uvicorn.\n",
      "\n",
      "Don't forget to mention that they can access the documentation at /docs. That's a big plus for FastAPI. Also, maybe mention that for production, they would use an ASGI server like Uvicorn.\n",
      "\n",
      "What about dependencies? They need to install FastAPI and Uvicorn. So include the pip install commands. Maybe also note that FastAPI is asynchronous, which can be beneficial for performance.\n",
      "\n",
      "Let me put this all together into a step-by-step guide with code examples. Make sure the code is correct and tested. For example, the Path parameter in the GET by ID should have a default value. Also, the update function should check if the item exists before updating.\n",
      "\n",
      "I think that's a solid approach. Now, structure the answer with the installation, code example, explanation of each part, and how to test it.\n",
      "</think>\n",
      "\n",
      "To create a REST API in Python, **FastAPI** is a modern, efficient choice with built-in support for OpenAPI/Swagger documentation and asynchronous capabilities. Below is a step-by-step guide with code examples.\n",
      "\n",
      "---\n",
      "\n",
      "### **Step 1: Install FastAPI and Uvicorn**\n",
      "```bash\n",
      "pip install fastapi uvicorn\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### **Step 2: Create a FastAPI Application**\n",
      "Create a file named `main.py`:\n",
      "\n",
      "```python\n",
      "from fastapi import FastAPI, Path, HTTPException\n",
      "from pydantic import BaseModel\n",
      "from typing import List, Optional\n",
      "\n",
      "app = FastAPI()\n",
      "\n",
      "# In-memory database\n",
      "todos = []\n",
      "\n",
      "# Pydantic model for request/response validation\n",
      "class Todo(BaseModel):\n",
      "    id: int\n",
      "    title: str\n",
      "    completed: Optional[bool] = False\n",
      "\n",
      "@app.get(\"/todos\", response_model=List[Todo])\n",
      "def get_all_todos():\n",
      "    return todos\n",
      "\n",
      "@app.get(\"/todos/{todo_id}\", response_model=Todo)\n",
      "def get_todo(todo_id: int = Path(..., description=\"ID of the todo to retrieve\")):\n",
      "    for todo in todos:\n",
      "        if todo.id == todo_id:\n",
      "            return todo\n",
      "    raise HTTPException(status_code=404, detail=\"Todo not found\")\n",
      "\n",
      "@app.post(\"/todos\", response_model=Todo)\n",
      "def create_todo(todo: Todo):\n",
      "    todos.append(todo)\n",
      "    return todo\n",
      "\n",
      "@app.put(\"/todos/{todo_id}\", response_model=Todo)\n",
      "def update_todo(todo_id: int, updated_todo: Todo):\n",
      "    for todo in todos:\n",
      "        if todo.id == todo_id:\n",
      "            todo.title = updated_todo.title\n",
      "            todo.completed = updated_todo.completed\n",
      "            return todo\n",
      "    raise HTTPException(status_code=404, detail=\"Todo not found\")\n",
      "\n",
      "@app.delete(\"/todos/{todo_id}\", response_model=Todo)\n",
      "def delete_todo(todo_id: int):\n",
      "    for i, todo in enumerate(todos):\n",
      "        if todo.id == todo_id:\n",
      "            del todos[i]\n",
      "            return todo\n",
      "    raise HTTPException(status_code=404, detail=\"Todo not found\")\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### **Step 3: Run the Server**\n",
      "```bash\n",
      "uvicorn main:app --reload\n",
      "```\n",
      "\n",
      "- Access the API docs at `http://localhost:8000/docs`.\n",
      "- The server runs on `http://localhost:8000`.\n",
      "\n",
      "---\n",
      "\n",
      "### **Example API Requests**\n",
      "\n",
      "1. **Create a Todo (POST)**  \n",
      "   ```bash\n",
      "   curl -X POST \"http://localhost:8000/todos\" -H \"Content-Type: application/json\" -d '{\"id\": 1, \"title\": \"Learn FastAPI\"}'\n",
      "   ```\n",
      "\n",
      "2. **Get All Todos (GET)**  \n",
      "   ```bash\n",
      "   curl http://localhost:8000/todos\n",
      "   ```\n",
      "\n",
      "3. **Update a Todo (PUT)**  \n",
      "   ```bash\n",
      "   curl -X PUT \"http://localhost:8000/todos/1\" -H \"Content-Type: application/json\" -d '{\"title\": \"Master FastAPI\", \"completed\": true}'\n",
      "   ```\n",
      "\n",
      "4. **Delete a Todo (DELETE)**  \n",
      "   ```bash\n",
      "   curl -X DELETE http://localhost:8000/todos/1\n",
      "   ```\n",
      "\n",
      "---\n",
      "\n",
      "### **Key Concepts**\n",
      "1. **Routing**: Use decorators like `@app.get`, `@app.post`, etc., to map HTTP methods to functions.\n",
      "2. **Request/Response Models**: Pydantic (`BaseModel`) ensures data validation and serialization.\n",
      "3. **Path Parameters**: Use `Path(...)` for parameter descriptions and validation.\n",
      "4. **Error Handling**: Raise `HTTPException` for custom error responses.\n",
      "5. **Documentation**: Auto-generated Swagger UI (`/docs`) for testing endpoints.\n",
      "\n",
      "---\n",
      "\n",
      "### **Production Deployment**\n",
      "- Use an ASGI server like **Uvicorn** or **Hypercorn**.\n",
      "- Deploy with a reverse proxy (e.g., **Nginx**) or cloud platforms (e.g., **Docker**, **Heroku**).\n",
      "\n",
      "This example provides a solid foundation for building a REST API in Python using FastAPI. Expand it with a real database (e.g., SQLAlchemy, MongoDB) for persistent storage.\n"
     ]
    }
   ],
   "source": [
    "## Detailed info to the LLM through System Message\n",
    "from langchain.messages import SystemMessage, HumanMessage\n",
    "sys_msg=SystemMessage(\"\"\" You are a senior Python developer with expertise in web frameworks.\n",
    "                      Always provide code examples and explain your reasoning.\n",
    "                      Be concise but thorough in your explanations.\n",
    "                      \"\"\")\n",
    "\n",
    "messages=[\n",
    "    sys_msg,\n",
    "    HumanMessage(\"How do I create a REST API?\")\n",
    "]\n",
    "\n",
    "response=model.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fa09716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user asked, \"whaat is 2+2?\" first. Let me start by answering that.\n",
      "\n",
      "2+2 is 4, right? That's basic arithmetic. But wait, maybe they made a typo in their question. The first message was \"Can you help me\" and I responded. Then they said \"great! whaat is 2+2?\" The word \"whaat\" seems like a typo for \"what\". So, they probably meant to ask \"what is 2+2?\".\n",
      "\n",
      "I should confirm that. Maybe they just misspelled \"what\". So, I'll answer 4 and maybe ask if they meant \"what\" in case there's more to the question. But since the question is straightforward, maybe there's no need. However, sometimes people ask 2+2 as a joke or for other contexts, like in the movie \"21\" where the answer is 3 but it's a trick question. Wait, no, in \"21\" the question is more about probability, not arithmetic. Hmm.\n",
      "\n",
      "Alternatively, maybe they want a more detailed explanation. But since it's 2+2, the answer is 4. I should keep it simple and clear. Also, since they corrected the typo in \"whaat\", maybe they're just testing or making sure the assistant can handle typos. Either way, the answer is 4. I'll present that and offer further help if needed.\n",
      "</think>\n",
      "\n",
      "Sure! The answer to 2 + 2 is **4**. Let me know if you need help with anything else! ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "from langchain.messages import AIMessage, SystemMessage, HumanMessage\n",
    "\n",
    "ai_message=AIMessage(\"I'd ne happy to help you with that question!\")\n",
    "\n",
    "messages=[\n",
    "    SystemMessage(\"You are a helpful assistance\"),\n",
    "    HumanMessage(\"Can you help me\"),\n",
    "    ai_message,\n",
    "    HumanMessage(\"great! whaat is 2+2 ?\")\n",
    "]\n",
    "response = model.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b609e3df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_tokens': 53, 'output_tokens': 332, 'total_tokens': 385}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96fa49a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import AIMessage\n",
    "from langchain.messages import ToolMessage\n",
    "\n",
    "ai_message=AIMessage(\n",
    "    content=[],\n",
    "    tool_calls=[{\n",
    "        \"name\":\"get_weather\",\n",
    "        \"args\":{\"location\":\"San Francisco\"},\n",
    "        \"id\":\"call_123\"\n",
    "    }]\n",
    ")\n",
    "\n",
    "weather_result=\"sunny, 72o F\"\n",
    "tool_message=ToolMessage(\n",
    "    content=weather_result,\n",
    "    tool_call_id=\"call_123\"\n",
    ")\n",
    "\n",
    "messages=[\n",
    "    HumanMessage(\"What's the weather in San Francisco?\"),\n",
    "    ai_message,\n",
    "    tool_message,\n",
    "]\n",
    "response=model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7240020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMessage(content='sunny, 72o F', tool_call_id='call_123')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16733ce7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-updates",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
