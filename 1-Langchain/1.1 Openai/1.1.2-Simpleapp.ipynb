{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33683f10",
   "metadata": {},
   "source": [
    "## Simple Gen AI App using LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b6cfdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "#Langsmith Tracking\n",
    "os.environ[\"LANGSMITH_API_KEY\"]=os.getenv(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGSMITH_TRACING\"]=\"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"]=os.getenv(\"LANGSMITH_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97a61972",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SAM\\Documents\\anjali\\Udemy-Agentic AI\\Langchain Project\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "## Data Ingestion --From the website  we need yo scrape the data\n",
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8109c4a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x25596d6fd00>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader=WebBaseLoader(\"https://docs.langchain.com/oss/python/langchain/agents\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1afb0d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='Agents - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationCore componentsAgentsLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartChangelogPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingStructured outputMiddlewareOverviewBuilt-in middlewareCustom middlewareAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryAgent developmentLangSmith StudioTestAgent Chat UIDeploy with LangSmithDeploymentObservabilityOn this pageCore componentsModelStatic modelDynamic modelToolsDefining toolsTool error handlingTool use in the ReAct loopDynamic toolsSystem promptDynamic system promptInvocationAdvanced conceptsStructured outputToolStrategyProviderStrategyMemoryDefining state via middlewareDefining state via state_schemaStreamingMiddlewareCore componentsAgentsCopy pageCopy pageAgents combine language models with tools to create systems that can reason about tasks, decide which tools to use, and iteratively work towards solutions.\\ncreate_agent provides a production-ready agent implementation.\\nAn LLM Agent runs tools in a loop to achieve a goal.\\nAn agent runs until a stop condition is met - i.e., when the model emits a final output or an iteration limit is reached.\\n\\ncreate_agent builds a graph-based agent runtime using LangGraph. A graph consists of nodes (steps) and edges (connections) that define how your agent processes information. The agent moves through this graph, executing nodes like the model node (which calls the model), the tools node (which executes tools), or middleware.Learn more about the Graph API.\\n\\u200bCore components\\n\\u200bModel\\nThe model is the reasoning engine of your agent. It can be specified in multiple ways, supporting both static and dynamic model selection.\\n\\u200bStatic model\\nStatic models are configured once when creating the agent and remain unchanged throughout execution. This is the most common and straightforward approach.\\nTo initialize a static model from a model identifier string:\\nCopyfrom langchain.agents import create_agent\\n\\nagent = create_agent(\"openai:gpt-5\", tools=tools)\\n\\nModel identifier strings support automatic inference (e.g., \"gpt-5\" will be inferred as \"openai:gpt-5\"). Refer to the reference to see a full list of model identifier string mappings.\\nFor more control over the model configuration, initialize a model instance directly using the provider package. In this example, we use ChatOpenAI. See Chat models for other available chat model classes.\\nCopyfrom langchain.agents import create_agent\\nfrom langchain_openai import ChatOpenAI\\n\\nmodel = ChatOpenAI(\\n    model=\"gpt-5\",\\n    temperature=0.1,\\n    max_tokens=1000,\\n    timeout=30\\n    # ... (other params)\\n)\\nagent = create_agent(model, tools=tools)\\n\\nModel instances give you complete control over configuration. Use them when you need to set specific parameters like temperature, max_tokens, timeouts, base_url, and other provider-specific settings. Refer to the reference to see available params and methods on your model.\\n\\u200bDynamic model\\nDynamic models are selected at runtime based on the current state and context. This enables sophisticated routing logic and cost optimization.\\nTo use a dynamic model, create middleware using the @wrap_model_call decorator that modifies the model in the request:\\nCopyfrom langchain_openai import ChatOpenAI\\nfrom langchain.agents import create_agent\\nfrom langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\\n\\n\\nbasic_model = ChatOpenAI(model=\"gpt-4o-mini\")\\nadvanced_model = ChatOpenAI(model=\"gpt-4o\")\\n\\n@wrap_model_call\\ndef dynamic_model_selection(request: ModelRequest, handler) -> ModelResponse:\\n    \"\"\"Choose model based on conversation complexity.\"\"\"\\n    message_count = len(request.state[\"messages\"])\\n\\n    if message_count > 10:\\n        # Use an advanced model for longer conversations\\n        model = advanced_model\\n    else:\\n        model = basic_model\\n\\n    return handler(request.override(model=model))\\n\\nagent = create_agent(\\n    model=basic_model,  # Default model\\n    tools=tools,\\n    middleware=[dynamic_model_selection]\\n)\\n\\nPre-bound models (models with bind_tools already called) are not supported when using structured output. If you need dynamic model selection with structured output, ensure the models passed to the middleware are not pre-bound.\\nFor model configuration details, see Models. For dynamic model selection patterns, see Dynamic model in middleware.\\n\\u200bTools\\nTools give agents the ability to take actions. Agents go beyond simple model-only tool binding by facilitating:\\n\\nMultiple tool calls in sequence (triggered by a single prompt)\\nParallel tool calls when appropriate\\nDynamic tool selection based on previous results\\nTool retry logic and error handling\\nState persistence across tool calls\\n\\nFor more information, see Tools.\\n\\u200bDefining tools\\nPass a list of tools to the agent.\\nTools can be specified as plain Python functions or coroutines.The tool decorator can be used to customize tool names, descriptions, argument schemas, and other properties.\\nCopyfrom langchain.tools import tool\\nfrom langchain.agents import create_agent\\n\\n\\n@tool\\ndef search(query: str) -> str:\\n    \"\"\"Search for information.\"\"\"\\n    return f\"Results for: {query}\"\\n\\n@tool\\ndef get_weather(location: str) -> str:\\n    \"\"\"Get weather information for a location.\"\"\"\\n    return f\"Weather in {location}: Sunny, 72°F\"\\n\\nagent = create_agent(model, tools=[search, get_weather])\\n\\nIf an empty tool list is provided, the agent will consist of a single LLM node without tool-calling capabilities.\\n\\u200bTool error handling\\nTo customize how tool errors are handled, use the @wrap_tool_call decorator to create middleware:\\nCopyfrom langchain.agents import create_agent\\nfrom langchain.agents.middleware import wrap_tool_call\\nfrom langchain.messages import ToolMessage\\n\\n\\n@wrap_tool_call\\ndef handle_tool_errors(request, handler):\\n    \"\"\"Handle tool execution errors with custom messages.\"\"\"\\n    try:\\n        return handler(request)\\n    except Exception as e:\\n        # Return a custom error message to the model\\n        return ToolMessage(\\n            content=f\"Tool error: Please check your input and try again. ({str(e)})\",\\n            tool_call_id=request.tool_call[\"id\"]\\n        )\\n\\nagent = create_agent(\\n    model=\"gpt-4o\",\\n    tools=[search, get_weather],\\n    middleware=[handle_tool_errors]\\n)\\n\\nThe agent will return a ToolMessage with the custom error message when a tool fails:\\nCopy[\\n    ...\\n    ToolMessage(\\n        content=\"Tool error: Please check your input and try again. (division by zero)\",\\n        tool_call_id=\"...\"\\n    ),\\n    ...\\n]\\n\\n\\u200bTool use in the ReAct loop\\nAgents follow the ReAct (“Reasoning + Acting”) pattern, alternating between brief reasoning steps with targeted tool calls and feeding the resulting observations into subsequent decisions until they can deliver a final answer.\\nExample of ReAct loopPrompt: Identify the current most popular wireless headphones and verify availability.Copy================================ Human Message =================================\\n\\nFind the most popular wireless headphones right now and check if they\\'re in stock\\n\\nReasoning: “Popularity is time-sensitive, I need to use the provided search tool.”\\nActing: Call search_products(\"wireless headphones\")\\nCopy================================== Ai Message ==================================\\nTool Calls:\\n  search_products (call_abc123)\\n Call ID: call_abc123\\n  Args:\\n    query: wireless headphones\\nCopy================================= Tool Message =================================\\n\\nFound 5 products matching \"wireless headphones\". Top 5 results: WH-1000XM5, ...\\n\\nReasoning: “I need to confirm availability for the top-ranked item before answering.”\\nActing: Call check_inventory(\"WH-1000XM5\")\\nCopy================================== Ai Message ==================================\\nTool Calls:\\n  check_inventory (call_def456)\\n Call ID: call_def456\\n  Args:\\n    product_id: WH-1000XM5\\nCopy================================= Tool Message =================================\\n\\nProduct WH-1000XM5: 10 units in stock\\n\\nReasoning: “I have the most popular model and its stock status. I can now answer the user’s question.”\\nActing: Produce final answer\\nCopy================================== Ai Message ==================================\\n\\nI found wireless headphones (model WH-1000XM5) with 10 units in stock...\\n\\n\\u200bDynamic tools\\nIn some scenarios, you need to modify the set of tools available to the agent at runtime rather than defining them all upfront. There are two approaches depending on whether tools are known ahead of time:\\n Filtering pre-registered tools Runtime tool registrationWhen all possible tools are known at agent creation time, you can pre-register them and dynamically filter which ones are exposed to the model based on state, permissions, or context.Copyfrom langchain.agents import create_agent\\nfrom langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\\nfrom typing import Callable\\n\\n@wrap_model_call\\ndef filter_tools(\\n    request: ModelRequest,\\n    handler: Callable[[ModelRequest], ModelResponse],\\n) -> ModelResponse:\\n    \"\"\"Filter tools based on user permissions.\"\"\"\\n    user_role = request.runtime.context.user_role\\n\\n    if user_role == \"admin\":\\n        # Admins get all tools\\n        tools = request.tools\\n    else:\\n        # Regular users get read-only tools\\n        tools = [t for t in request.tools if t.name.startswith(\"read_\")]\\n\\n    return handler(request.override(tools=tools))\\n\\nagent = create_agent(\\n    model=\"gpt-4o\",\\n    tools=[read_data, write_data, delete_data],  # All tools pre-registered\\n    middleware=[filter_tools],\\n)\\nThis approach is best when:\\nAll possible tools are known at compile/startup time\\nYou want to filter based on permissions, feature flags, or conversation state\\nTools are static but their availability is dynamic\\nSee Dynamically selecting tools for more examples.When tools are discovered or created at runtime (e.g., loaded from an MCP server, generated based on user data, or fetched from a remote registry), you need to both register the tools and handle their execution dynamically.This requires two middleware hooks:\\nwrap_model_call - Add the dynamic tools to the request\\nwrap_tool_call - Handle execution of the dynamically added tools\\nCopyfrom langchain.tools import tool\\nfrom langchain.agents import create_agent\\nfrom langchain.agents.middleware import AgentMiddleware, ModelRequest\\nfrom langchain.agents.middleware.types import ToolCallRequest\\n\\n# A tool that will be added dynamically at runtime\\n@tool\\ndef calculate_tip(bill_amount: float, tip_percentage: float = 20.0) -> str:\\n    \"\"\"Calculate the tip amount for a bill.\"\"\"\\n    tip = bill_amount * (tip_percentage / 100)\\n    return f\"Tip: ${tip:.2f}, Total: ${bill_amount + tip:.2f}\"\\n\\nclass DynamicToolMiddleware(AgentMiddleware):\\n    \"\"\"Middleware that registers and handles dynamic tools.\"\"\"\\n\\n    def wrap_model_call(self, request: ModelRequest, handler):\\n        # Add dynamic tool to the request\\n        # This could be loaded from an MCP server, database, etc.\\n        updated = request.override(tools=[*request.tools, calculate_tip])\\n        return handler(updated)\\n\\n    def wrap_tool_call(self, request: ToolCallRequest, handler):\\n        # Handle execution of the dynamic tool\\n        if request.tool_call[\"name\"] == \"calculate_tip\":\\n            return handler(request.override(tool=calculate_tip))\\n        return handler(request)\\n\\nagent = create_agent(\\n    model=\"gpt-4o\",\\n    tools=[get_weather],  # Only static tools registered here\\n    middleware=[DynamicToolMiddleware()],\\n)\\n\\n# The agent can now use both get_weather AND calculate_tip\\nresult = agent.invoke({\\n    \"messages\": [{\"role\": \"user\", \"content\": \"Calculate a 20% tip on $85\"}]\\n})\\nThis approach is best when:\\nTools are discovered at runtime (e.g., from an MCP server)\\nTools are generated dynamically based on user data or configuration\\nYou’re integrating with external tool registries\\nThe wrap_tool_call hook is required for runtime-registered tools because the agent needs to know how to execute tools that weren’t in the original tool list. Without it, the agent won’t know how to invoke the dynamically added tool.\\nTo learn more about tools, see Tools.\\n\\u200bSystem prompt\\nYou can shape how your agent approaches tasks by providing a prompt. The system_prompt parameter can be provided as a string:\\nCopyagent = create_agent(\\n    model,\\n    tools,\\n    system_prompt=\"You are a helpful assistant. Be concise and accurate.\"\\n)\\n\\nWhen no system_prompt is provided, the agent will infer its task from the messages directly.\\nThe system_prompt parameter accepts either a str or a SystemMessage. Using a SystemMessage gives you more control over the prompt structure, which is useful for provider-specific features like Anthropic’s prompt caching:\\nCopyfrom langchain.agents import create_agent\\nfrom langchain.messages import SystemMessage, HumanMessage\\n\\nliterary_agent = create_agent(\\n    model=\"anthropic:claude-sonnet-4-5\",\\n    system_prompt=SystemMessage(\\n        content=[\\n            {\\n                \"type\": \"text\",\\n                \"text\": \"You are an AI assistant tasked with analyzing literary works.\",\\n            },\\n            {\\n                \"type\": \"text\",\\n                \"text\": \"<the entire contents of \\'Pride and Prejudice\\'>\",\\n                \"cache_control\": {\"type\": \"ephemeral\"}\\n            }\\n        ]\\n    )\\n)\\n\\nresult = literary_agent.invoke(\\n    {\"messages\": [HumanMessage(\"Analyze the major themes in \\'Pride and Prejudice\\'.\")]}\\n)\\n\\nThe cache_control field with {\"type\": \"ephemeral\"} tells Anthropic to cache that content block, reducing latency and costs for repeated requests that use the same system prompt.\\n\\u200bDynamic system prompt\\nFor more advanced use cases where you need to modify the system prompt based on runtime context or agent state, you can use middleware.\\nThe @dynamic_prompt decorator creates middleware that generates system prompts based on the model request:\\nCopyfrom typing import TypedDict\\n\\nfrom langchain.agents import create_agent\\nfrom langchain.agents.middleware import dynamic_prompt, ModelRequest\\n\\n\\nclass Context(TypedDict):\\n    user_role: str\\n\\n@dynamic_prompt\\ndef user_role_prompt(request: ModelRequest) -> str:\\n    \"\"\"Generate system prompt based on user role.\"\"\"\\n    user_role = request.runtime.context.get(\"user_role\", \"user\")\\n    base_prompt = \"You are a helpful assistant.\"\\n\\n    if user_role == \"expert\":\\n        return f\"{base_prompt} Provide detailed technical responses.\"\\n    elif user_role == \"beginner\":\\n        return f\"{base_prompt} Explain concepts simply and avoid jargon.\"\\n\\n    return base_prompt\\n\\nagent = create_agent(\\n    model=\"gpt-4o\",\\n    tools=[web_search],\\n    middleware=[user_role_prompt],\\n    context_schema=Context\\n)\\n\\n# The system prompt will be set dynamically based on context\\nresult = agent.invoke(\\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Explain machine learning\"}]},\\n    context={\"user_role\": \"expert\"}\\n)\\n\\nFor more details on message types and formatting, see Messages. For comprehensive middleware documentation, see Middleware.\\n\\u200bInvocation\\nYou can invoke an agent by passing an update to its State. All agents include a sequence of messages in their state; to invoke the agent, pass a new message:\\nCopyresult = agent.invoke(\\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What\\'s the weather in San Francisco?\"}]}\\n)\\n\\nFor streaming steps and / or tokens from the agent, refer to the streaming guide.\\nOtherwise, the agent follows the LangGraph Graph API and supports all associated methods, such as stream and invoke.\\n\\u200bAdvanced concepts\\n\\u200bStructured output\\nIn some situations, you may want the agent to return an output in a specific format. LangChain provides strategies for structured output via the response_format parameter.\\n\\u200bToolStrategy\\nToolStrategy uses artificial tool calling to generate structured output. This works with any model that supports tool calling. ToolStrategy should be used when provider-native structured output (via ProviderStrategy) is not available or reliable.\\nCopyfrom pydantic import BaseModel\\nfrom langchain.agents import create_agent\\nfrom langchain.agents.structured_output import ToolStrategy\\n\\n\\nclass ContactInfo(BaseModel):\\n    name: str\\n    email: str\\n    phone: str\\n\\nagent = create_agent(\\n    model=\"gpt-4o-mini\",\\n    tools=[search_tool],\\n    response_format=ToolStrategy(ContactInfo)\\n)\\n\\nresult = agent.invoke({\\n    \"messages\": [{\"role\": \"user\", \"content\": \"Extract contact info from: John Doe, [email\\xa0protected], (555) 123-4567\"}]\\n})\\n\\nresult[\"structured_response\"]\\n# ContactInfo(name=\\'John Doe\\', email=\\'[email\\xa0protected]\\', phone=\\'(555) 123-4567\\')\\n\\n\\u200bProviderStrategy\\nProviderStrategy uses the model provider’s native structured output generation. This is more reliable but only works with providers that support native structured output:\\nCopyfrom langchain.agents.structured_output import ProviderStrategy\\n\\nagent = create_agent(\\n    model=\"gpt-4o\",\\n    response_format=ProviderStrategy(ContactInfo)\\n)\\n\\nAs of langchain 1.0, simply passing a schema (e.g., response_format=ContactInfo) will default to ProviderStrategy if the model supports native structured output. It will fall back to ToolStrategy otherwise.\\nTo learn about structured output, see Structured output.\\n\\u200bMemory\\nAgents maintain conversation history automatically through the message state. You can also configure the agent to use a custom state schema to remember additional information during the conversation.\\nInformation stored in the state can be thought of as the short-term memory of the agent:\\nCustom state schemas must extend AgentState as a TypedDict.\\nThere are two ways to define custom state:\\n\\nVia middleware (preferred)\\nVia state_schema on create_agent\\n\\n\\u200bDefining state via middleware\\nUse middleware to define custom state when your custom state needs to be accessed by specific middleware hooks and tools attached to said middleware.\\nCopyfrom langchain.agents import AgentState\\nfrom langchain.agents.middleware import AgentMiddleware\\nfrom typing import Any\\n\\n\\nclass CustomState(AgentState):\\n    user_preferences: dict\\n\\nclass CustomMiddleware(AgentMiddleware):\\n    state_schema = CustomState\\n    tools = [tool1, tool2]\\n\\n    def before_model(self, state: CustomState, runtime) -> dict[str, Any] | None:\\n        ...\\n\\nagent = create_agent(\\n    model,\\n    tools=tools,\\n    middleware=[CustomMiddleware()]\\n)\\n\\n# The agent can now track additional state beyond messages\\nresult = agent.invoke({\\n    \"messages\": [{\"role\": \"user\", \"content\": \"I prefer technical explanations\"}],\\n    \"user_preferences\": {\"style\": \"technical\", \"verbosity\": \"detailed\"},\\n})\\n\\n\\u200bDefining state via state_schema\\nUse the state_schema parameter as a shortcut to define custom state that is only used in tools.\\nCopyfrom langchain.agents import AgentState\\n\\n\\nclass CustomState(AgentState):\\n    user_preferences: dict\\n\\nagent = create_agent(\\n    model,\\n    tools=[tool1, tool2],\\n    state_schema=CustomState\\n)\\n# The agent can now track additional state beyond messages\\nresult = agent.invoke({\\n    \"messages\": [{\"role\": \"user\", \"content\": \"I prefer technical explanations\"}],\\n    \"user_preferences\": {\"style\": \"technical\", \"verbosity\": \"detailed\"},\\n})\\n\\nAs of langchain 1.0, custom state schemas must be TypedDict types. Pydantic models and dataclasses are no longer supported. See the v1 migration guide for more details.\\nDefining custom state via middleware is preferred over defining it via state_schema on create_agent because it allows you to keep state extensions conceptually scoped to the relevant middleware and tools.state_schema is still supported for backwards compatibility on create_agent.\\nTo learn more about memory, see Memory. For information on implementing long-term memory that persists across sessions, see Long-term memory.\\n\\u200bStreaming\\nWe’ve seen how the agent can be called with invoke to get a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.\\nCopyfor chunk in agent.stream({\\n    \"messages\": [{\"role\": \"user\", \"content\": \"Search for AI news and summarize the findings\"}]\\n}, stream_mode=\"values\"):\\n    # Each chunk contains the full state at that point\\n    latest_message = chunk[\"messages\"][-1]\\n    if latest_message.content:\\n        print(f\"Agent: {latest_message.content}\")\\n    elif latest_message.tool_calls:\\n        print(f\"Calling tools: {[tc[\\'name\\'] for tc in latest_message.tool_calls]}\")\\n\\nFor more details on streaming, see Streaming.\\n\\u200bMiddleware\\nMiddleware provides powerful extensibility for customizing agent behavior at different stages of execution. You can use middleware to:\\n\\nProcess state before the model is called (e.g., message trimming, context injection)\\nModify or validate the model’s response (e.g., guardrails, content filtering)\\nHandle tool execution errors with custom logic\\nImplement dynamic model selection based on state or context\\nAdd custom logging, monitoring, or analytics\\n\\nMiddleware integrates seamlessly into the agent’s execution, allowing you to intercept and modify data flow at key points without changing the core agent logic.\\nFor comprehensive middleware documentation including decorators like @before_model, @after_model, and @wrap_tool_call, see Middleware.\\n\\nEdit this page on GitHub or file an issue.\\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoPhilosophyPreviousModelsNext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by\\n')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs=loader.load()\n",
    "docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5453f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load Data--> Docs--> Divide our text into chunks ---> text -->vectors\n",
    "#  using Vector Embedding(techninques convert text into vectors)\n",
    "# --> Vector store DB\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "documents=text_splitter.split_documents(docs) #chunks of my docs\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c45db7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='Agents - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationCore componentsAgentsLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartChangelogPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingStructured outputMiddlewareOverviewBuilt-in middlewareCustom middlewareAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryAgent developmentLangSmith StudioTestAgent Chat UIDeploy with LangSmithDeploymentObservabilityOn this pageCore componentsModelStatic modelDynamic modelToolsDefining toolsTool error handlingTool use in the ReAct loopDynamic toolsSystem promptDynamic system promptInvocationAdvanced conceptsStructured outputToolStrategyProviderStrategyMemoryDefining state via middlewareDefining state via state_schemaStreamingMiddlewareCore'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='promptDynamic system promptInvocationAdvanced conceptsStructured outputToolStrategyProviderStrategyMemoryDefining state via middlewareDefining state via state_schemaStreamingMiddlewareCore componentsAgentsCopy pageCopy pageAgents combine language models with tools to create systems that can reason about tasks, decide which tools to use, and iteratively work towards solutions.'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='create_agent provides a production-ready agent implementation.\\nAn LLM Agent runs tools in a loop to achieve a goal.\\nAn agent runs until a stop condition is met - i.e., when the model emits a final output or an iteration limit is reached.'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='create_agent builds a graph-based agent runtime using LangGraph. A graph consists of nodes (steps) and edges (connections) that define how your agent processes information. The agent moves through this graph, executing nodes like the model node (which calls the model), the tools node (which executes tools), or middleware.Learn more about the Graph API.\\n\\u200bCore components\\n\\u200bModel\\nThe model is the reasoning engine of your agent. It can be specified in multiple ways, supporting both static and dynamic model selection.\\n\\u200bStatic model\\nStatic models are configured once when creating the agent and remain unchanged throughout execution. This is the most common and straightforward approach.\\nTo initialize a static model from a model identifier string:\\nCopyfrom langchain.agents import create_agent\\n\\nagent = create_agent(\"openai:gpt-5\", tools=tools)'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='agent = create_agent(\"openai:gpt-5\", tools=tools)\\n\\nModel identifier strings support automatic inference (e.g., \"gpt-5\" will be inferred as \"openai:gpt-5\"). Refer to the reference to see a full list of model identifier string mappings.\\nFor more control over the model configuration, initialize a model instance directly using the provider package. In this example, we use ChatOpenAI. See Chat models for other available chat model classes.\\nCopyfrom langchain.agents import create_agent\\nfrom langchain_openai import ChatOpenAI\\n\\nmodel = ChatOpenAI(\\n    model=\"gpt-5\",\\n    temperature=0.1,\\n    max_tokens=1000,\\n    timeout=30\\n    # ... (other params)\\n)\\nagent = create_agent(model, tools=tools)'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='model = ChatOpenAI(\\n    model=\"gpt-5\",\\n    temperature=0.1,\\n    max_tokens=1000,\\n    timeout=30\\n    # ... (other params)\\n)\\nagent = create_agent(model, tools=tools)\\n\\nModel instances give you complete control over configuration. Use them when you need to set specific parameters like temperature, max_tokens, timeouts, base_url, and other provider-specific settings. Refer to the reference to see available params and methods on your model.\\n\\u200bDynamic model\\nDynamic models are selected at runtime based on the current state and context. This enables sophisticated routing logic and cost optimization.\\nTo use a dynamic model, create middleware using the @wrap_model_call decorator that modifies the model in the request:\\nCopyfrom langchain_openai import ChatOpenAI\\nfrom langchain.agents import create_agent\\nfrom langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\\n\\n\\nbasic_model = ChatOpenAI(model=\"gpt-4o-mini\")\\nadvanced_model = ChatOpenAI(model=\"gpt-4o\")'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='basic_model = ChatOpenAI(model=\"gpt-4o-mini\")\\nadvanced_model = ChatOpenAI(model=\"gpt-4o\")\\n\\n@wrap_model_call\\ndef dynamic_model_selection(request: ModelRequest, handler) -> ModelResponse:\\n    \"\"\"Choose model based on conversation complexity.\"\"\"\\n    message_count = len(request.state[\"messages\"])\\n\\n    if message_count > 10:\\n        # Use an advanced model for longer conversations\\n        model = advanced_model\\n    else:\\n        model = basic_model\\n\\n    return handler(request.override(model=model))\\n\\nagent = create_agent(\\n    model=basic_model,  # Default model\\n    tools=tools,\\n    middleware=[dynamic_model_selection]\\n)'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='return handler(request.override(model=model))\\n\\nagent = create_agent(\\n    model=basic_model,  # Default model\\n    tools=tools,\\n    middleware=[dynamic_model_selection]\\n)\\n\\nPre-bound models (models with bind_tools already called) are not supported when using structured output. If you need dynamic model selection with structured output, ensure the models passed to the middleware are not pre-bound.\\nFor model configuration details, see Models. For dynamic model selection patterns, see Dynamic model in middleware.\\n\\u200bTools\\nTools give agents the ability to take actions. Agents go beyond simple model-only tool binding by facilitating:\\n\\nMultiple tool calls in sequence (triggered by a single prompt)\\nParallel tool calls when appropriate\\nDynamic tool selection based on previous results\\nTool retry logic and error handling\\nState persistence across tool calls'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='For more information, see Tools.\\n\\u200bDefining tools\\nPass a list of tools to the agent.\\nTools can be specified as plain Python functions or coroutines.The tool decorator can be used to customize tool names, descriptions, argument schemas, and other properties.\\nCopyfrom langchain.tools import tool\\nfrom langchain.agents import create_agent\\n\\n\\n@tool\\ndef search(query: str) -> str:\\n    \"\"\"Search for information.\"\"\"\\n    return f\"Results for: {query}\"\\n\\n@tool\\ndef get_weather(location: str) -> str:\\n    \"\"\"Get weather information for a location.\"\"\"\\n    return f\"Weather in {location}: Sunny, 72°F\"\\n\\nagent = create_agent(model, tools=[search, get_weather])'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='agent = create_agent(model, tools=[search, get_weather])\\n\\nIf an empty tool list is provided, the agent will consist of a single LLM node without tool-calling capabilities.\\n\\u200bTool error handling\\nTo customize how tool errors are handled, use the @wrap_tool_call decorator to create middleware:\\nCopyfrom langchain.agents import create_agent\\nfrom langchain.agents.middleware import wrap_tool_call\\nfrom langchain.messages import ToolMessage\\n\\n\\n@wrap_tool_call\\ndef handle_tool_errors(request, handler):\\n    \"\"\"Handle tool execution errors with custom messages.\"\"\"\\n    try:\\n        return handler(request)\\n    except Exception as e:\\n        # Return a custom error message to the model\\n        return ToolMessage(\\n            content=f\"Tool error: Please check your input and try again. ({str(e)})\",\\n            tool_call_id=request.tool_call[\"id\"]\\n        )\\n\\nagent = create_agent(\\n    model=\"gpt-4o\",\\n    tools=[search, get_weather],\\n    middleware=[handle_tool_errors]\\n)'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='agent = create_agent(\\n    model=\"gpt-4o\",\\n    tools=[search, get_weather],\\n    middleware=[handle_tool_errors]\\n)\\n\\nThe agent will return a ToolMessage with the custom error message when a tool fails:\\nCopy[\\n    ...\\n    ToolMessage(\\n        content=\"Tool error: Please check your input and try again. (division by zero)\",\\n        tool_call_id=\"...\"\\n    ),\\n    ...\\n]\\n\\n\\u200bTool use in the ReAct loop\\nAgents follow the ReAct (“Reasoning + Acting”) pattern, alternating between brief reasoning steps with targeted tool calls and feeding the resulting observations into subsequent decisions until they can deliver a final answer.\\nExample of ReAct loopPrompt: Identify the current most popular wireless headphones and verify availability.Copy================================ Human Message =================================\\n\\nFind the most popular wireless headphones right now and check if they\\'re in stock'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='Find the most popular wireless headphones right now and check if they\\'re in stock\\n\\nReasoning: “Popularity is time-sensitive, I need to use the provided search tool.”\\nActing: Call search_products(\"wireless headphones\")\\nCopy================================== Ai Message ==================================\\nTool Calls:\\n  search_products (call_abc123)\\n Call ID: call_abc123\\n  Args:\\n    query: wireless headphones\\nCopy================================= Tool Message =================================\\n\\nFound 5 products matching \"wireless headphones\". Top 5 results: WH-1000XM5, ...\\n\\nReasoning: “I need to confirm availability for the top-ranked item before answering.”\\nActing: Call check_inventory(\"WH-1000XM5\")\\nCopy================================== Ai Message ==================================\\nTool Calls:\\n  check_inventory (call_def456)\\n Call ID: call_def456\\n  Args:\\n    product_id: WH-1000XM5\\nCopy================================= Tool Message ================================='),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='Product WH-1000XM5: 10 units in stock\\n\\nReasoning: “I have the most popular model and its stock status. I can now answer the user’s question.”\\nActing: Produce final answer\\nCopy================================== Ai Message ==================================\\n\\nI found wireless headphones (model WH-1000XM5) with 10 units in stock...\\n\\n\\u200bDynamic tools\\nIn some scenarios, you need to modify the set of tools available to the agent at runtime rather than defining them all upfront. There are two approaches depending on whether tools are known ahead of time:\\n Filtering pre-registered tools Runtime tool registrationWhen all possible tools are known at agent creation time, you can pre-register them and dynamically filter which ones are exposed to the model based on state, permissions, or context.Copyfrom langchain.agents import create_agent\\nfrom langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\\nfrom typing import Callable'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='@wrap_model_call\\ndef filter_tools(\\n    request: ModelRequest,\\n    handler: Callable[[ModelRequest], ModelResponse],\\n) -> ModelResponse:\\n    \"\"\"Filter tools based on user permissions.\"\"\"\\n    user_role = request.runtime.context.user_role\\n\\n    if user_role == \"admin\":\\n        # Admins get all tools\\n        tools = request.tools\\n    else:\\n        # Regular users get read-only tools\\n        tools = [t for t in request.tools if t.name.startswith(\"read_\")]\\n\\n    return handler(request.override(tools=tools))'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='agent = create_agent(\\n    model=\"gpt-4o\",\\n    tools=[read_data, write_data, delete_data],  # All tools pre-registered\\n    middleware=[filter_tools],\\n)\\nThis approach is best when:\\nAll possible tools are known at compile/startup time\\nYou want to filter based on permissions, feature flags, or conversation state\\nTools are static but their availability is dynamic\\nSee Dynamically selecting tools for more examples.When tools are discovered or created at runtime (e.g., loaded from an MCP server, generated based on user data, or fetched from a remote registry), you need to both register the tools and handle their execution dynamically.This requires two middleware hooks:\\nwrap_model_call - Add the dynamic tools to the request\\nwrap_tool_call - Handle execution of the dynamically added tools\\nCopyfrom langchain.tools import tool\\nfrom langchain.agents import create_agent\\nfrom langchain.agents.middleware import AgentMiddleware, ModelRequest'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='Copyfrom langchain.tools import tool\\nfrom langchain.agents import create_agent\\nfrom langchain.agents.middleware import AgentMiddleware, ModelRequest\\nfrom langchain.agents.middleware.types import ToolCallRequest'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='# A tool that will be added dynamically at runtime\\n@tool\\ndef calculate_tip(bill_amount: float, tip_percentage: float = 20.0) -> str:\\n    \"\"\"Calculate the tip amount for a bill.\"\"\"\\n    tip = bill_amount * (tip_percentage / 100)\\n    return f\"Tip: ${tip:.2f}, Total: ${bill_amount + tip:.2f}\"\\n\\nclass DynamicToolMiddleware(AgentMiddleware):\\n    \"\"\"Middleware that registers and handles dynamic tools.\"\"\"\\n\\n    def wrap_model_call(self, request: ModelRequest, handler):\\n        # Add dynamic tool to the request\\n        # This could be loaded from an MCP server, database, etc.\\n        updated = request.override(tools=[*request.tools, calculate_tip])\\n        return handler(updated)\\n\\n    def wrap_tool_call(self, request: ToolCallRequest, handler):\\n        # Handle execution of the dynamic tool\\n        if request.tool_call[\"name\"] == \"calculate_tip\":\\n            return handler(request.override(tool=calculate_tip))\\n        return handler(request)'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='agent = create_agent(\\n    model=\"gpt-4o\",\\n    tools=[get_weather],  # Only static tools registered here\\n    middleware=[DynamicToolMiddleware()],\\n)'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='# The agent can now use both get_weather AND calculate_tip\\nresult = agent.invoke({\\n    \"messages\": [{\"role\": \"user\", \"content\": \"Calculate a 20% tip on $85\"}]\\n})\\nThis approach is best when:\\nTools are discovered at runtime (e.g., from an MCP server)\\nTools are generated dynamically based on user data or configuration\\nYou’re integrating with external tool registries\\nThe wrap_tool_call hook is required for runtime-registered tools because the agent needs to know how to execute tools that weren’t in the original tool list. Without it, the agent won’t know how to invoke the dynamically added tool.\\nTo learn more about tools, see Tools.\\n\\u200bSystem prompt\\nYou can shape how your agent approaches tasks by providing a prompt. The system_prompt parameter can be provided as a string:\\nCopyagent = create_agent(\\n    model,\\n    tools,\\n    system_prompt=\"You are a helpful assistant. Be concise and accurate.\"\\n)'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='When no system_prompt is provided, the agent will infer its task from the messages directly.\\nThe system_prompt parameter accepts either a str or a SystemMessage. Using a SystemMessage gives you more control over the prompt structure, which is useful for provider-specific features like Anthropic’s prompt caching:\\nCopyfrom langchain.agents import create_agent\\nfrom langchain.messages import SystemMessage, HumanMessage\\n\\nliterary_agent = create_agent(\\n    model=\"anthropic:claude-sonnet-4-5\",\\n    system_prompt=SystemMessage(\\n        content=[\\n            {\\n                \"type\": \"text\",\\n                \"text\": \"You are an AI assistant tasked with analyzing literary works.\",\\n            },\\n            {\\n                \"type\": \"text\",\\n                \"text\": \"<the entire contents of \\'Pride and Prejudice\\'>\",\\n                \"cache_control\": {\"type\": \"ephemeral\"}\\n            }\\n        ]\\n    )\\n)'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='result = literary_agent.invoke(\\n    {\"messages\": [HumanMessage(\"Analyze the major themes in \\'Pride and Prejudice\\'.\")]}\\n)\\n\\nThe cache_control field with {\"type\": \"ephemeral\"} tells Anthropic to cache that content block, reducing latency and costs for repeated requests that use the same system prompt.\\n\\u200bDynamic system prompt\\nFor more advanced use cases where you need to modify the system prompt based on runtime context or agent state, you can use middleware.\\nThe @dynamic_prompt decorator creates middleware that generates system prompts based on the model request:\\nCopyfrom typing import TypedDict\\n\\nfrom langchain.agents import create_agent\\nfrom langchain.agents.middleware import dynamic_prompt, ModelRequest\\n\\n\\nclass Context(TypedDict):\\n    user_role: str\\n\\n@dynamic_prompt\\ndef user_role_prompt(request: ModelRequest) -> str:\\n    \"\"\"Generate system prompt based on user role.\"\"\"\\n    user_role = request.runtime.context.get(\"user_role\", \"user\")\\n    base_prompt = \"You are a helpful assistant.\"'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='if user_role == \"expert\":\\n        return f\"{base_prompt} Provide detailed technical responses.\"\\n    elif user_role == \"beginner\":\\n        return f\"{base_prompt} Explain concepts simply and avoid jargon.\"\\n\\n    return base_prompt\\n\\nagent = create_agent(\\n    model=\"gpt-4o\",\\n    tools=[web_search],\\n    middleware=[user_role_prompt],\\n    context_schema=Context\\n)\\n\\n# The system prompt will be set dynamically based on context\\nresult = agent.invoke(\\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Explain machine learning\"}]},\\n    context={\"user_role\": \"expert\"}\\n)\\n\\nFor more details on message types and formatting, see Messages. For comprehensive middleware documentation, see Middleware.\\n\\u200bInvocation\\nYou can invoke an agent by passing an update to its State. All agents include a sequence of messages in their state; to invoke the agent, pass a new message:\\nCopyresult = agent.invoke(\\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What\\'s the weather in San Francisco?\"}]}\\n)'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='For streaming steps and / or tokens from the agent, refer to the streaming guide.\\nOtherwise, the agent follows the LangGraph Graph API and supports all associated methods, such as stream and invoke.\\n\\u200bAdvanced concepts\\n\\u200bStructured output\\nIn some situations, you may want the agent to return an output in a specific format. LangChain provides strategies for structured output via the response_format parameter.\\n\\u200bToolStrategy\\nToolStrategy uses artificial tool calling to generate structured output. This works with any model that supports tool calling. ToolStrategy should be used when provider-native structured output (via ProviderStrategy) is not available or reliable.\\nCopyfrom pydantic import BaseModel\\nfrom langchain.agents import create_agent\\nfrom langchain.agents.structured_output import ToolStrategy\\n\\n\\nclass ContactInfo(BaseModel):\\n    name: str\\n    email: str\\n    phone: str'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='class ContactInfo(BaseModel):\\n    name: str\\n    email: str\\n    phone: str\\n\\nagent = create_agent(\\n    model=\"gpt-4o-mini\",\\n    tools=[search_tool],\\n    response_format=ToolStrategy(ContactInfo)\\n)\\n\\nresult = agent.invoke({\\n    \"messages\": [{\"role\": \"user\", \"content\": \"Extract contact info from: John Doe, [email\\xa0protected], (555) 123-4567\"}]\\n})\\n\\nresult[\"structured_response\"]\\n# ContactInfo(name=\\'John Doe\\', email=\\'[email\\xa0protected]\\', phone=\\'(555) 123-4567\\')\\n\\n\\u200bProviderStrategy\\nProviderStrategy uses the model provider’s native structured output generation. This is more reliable but only works with providers that support native structured output:\\nCopyfrom langchain.agents.structured_output import ProviderStrategy\\n\\nagent = create_agent(\\n    model=\"gpt-4o\",\\n    response_format=ProviderStrategy(ContactInfo)\\n)'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='agent = create_agent(\\n    model=\"gpt-4o\",\\n    response_format=ProviderStrategy(ContactInfo)\\n)\\n\\nAs of langchain 1.0, simply passing a schema (e.g., response_format=ContactInfo) will default to ProviderStrategy if the model supports native structured output. It will fall back to ToolStrategy otherwise.\\nTo learn about structured output, see Structured output.\\n\\u200bMemory\\nAgents maintain conversation history automatically through the message state. You can also configure the agent to use a custom state schema to remember additional information during the conversation.\\nInformation stored in the state can be thought of as the short-term memory of the agent:\\nCustom state schemas must extend AgentState as a TypedDict.\\nThere are two ways to define custom state:\\n\\nVia middleware (preferred)\\nVia state_schema on create_agent'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='Via middleware (preferred)\\nVia state_schema on create_agent\\n\\n\\u200bDefining state via middleware\\nUse middleware to define custom state when your custom state needs to be accessed by specific middleware hooks and tools attached to said middleware.\\nCopyfrom langchain.agents import AgentState\\nfrom langchain.agents.middleware import AgentMiddleware\\nfrom typing import Any\\n\\n\\nclass CustomState(AgentState):\\n    user_preferences: dict\\n\\nclass CustomMiddleware(AgentMiddleware):\\n    state_schema = CustomState\\n    tools = [tool1, tool2]\\n\\n    def before_model(self, state: CustomState, runtime) -> dict[str, Any] | None:\\n        ...\\n\\nagent = create_agent(\\n    model,\\n    tools=tools,\\n    middleware=[CustomMiddleware()]\\n)\\n\\n# The agent can now track additional state beyond messages\\nresult = agent.invoke({\\n    \"messages\": [{\"role\": \"user\", \"content\": \"I prefer technical explanations\"}],\\n    \"user_preferences\": {\"style\": \"technical\", \"verbosity\": \"detailed\"},\\n})'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='\\u200bDefining state via state_schema\\nUse the state_schema parameter as a shortcut to define custom state that is only used in tools.\\nCopyfrom langchain.agents import AgentState\\n\\n\\nclass CustomState(AgentState):\\n    user_preferences: dict\\n\\nagent = create_agent(\\n    model,\\n    tools=[tool1, tool2],\\n    state_schema=CustomState\\n)\\n# The agent can now track additional state beyond messages\\nresult = agent.invoke({\\n    \"messages\": [{\"role\": \"user\", \"content\": \"I prefer technical explanations\"}],\\n    \"user_preferences\": {\"style\": \"technical\", \"verbosity\": \"detailed\"},\\n})'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='As of langchain 1.0, custom state schemas must be TypedDict types. Pydantic models and dataclasses are no longer supported. See the v1 migration guide for more details.\\nDefining custom state via middleware is preferred over defining it via state_schema on create_agent because it allows you to keep state extensions conceptually scoped to the relevant middleware and tools.state_schema is still supported for backwards compatibility on create_agent.\\nTo learn more about memory, see Memory. For information on implementing long-term memory that persists across sessions, see Long-term memory.\\n\\u200bStreaming\\nWe’ve seen how the agent can be called with invoke to get a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.\\nCopyfor chunk in agent.stream({\\n    \"messages\": [{\"role\": \"user\", \"content\": \"Search for AI news and summarize the findings\"}]\\n}, stream_mode=\"values\"):'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='Copyfor chunk in agent.stream({\\n    \"messages\": [{\"role\": \"user\", \"content\": \"Search for AI news and summarize the findings\"}]\\n}, stream_mode=\"values\"):\\n    # Each chunk contains the full state at that point\\n    latest_message = chunk[\"messages\"][-1]\\n    if latest_message.content:\\n        print(f\"Agent: {latest_message.content}\")\\n    elif latest_message.tool_calls:\\n        print(f\"Calling tools: {[tc[\\'name\\'] for tc in latest_message.tool_calls]}\")'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='For more details on streaming, see Streaming.\\n\\u200bMiddleware\\nMiddleware provides powerful extensibility for customizing agent behavior at different stages of execution. You can use middleware to:\\n\\nProcess state before the model is called (e.g., message trimming, context injection)\\nModify or validate the model’s response (e.g., guardrails, content filtering)\\nHandle tool execution errors with custom logic\\nImplement dynamic model selection based on state or context\\nAdd custom logging, monitoring, or analytics\\n\\nMiddleware integrates seamlessly into the agent’s execution, allowing you to intercept and modify data flow at key points without changing the core agent logic.\\nFor comprehensive middleware documentation including decorators like @before_model, @after_model, and @wrap_tool_call, see Middleware.'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='Edit this page on GitHub or file an issue.\\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoPhilosophyPreviousModelsNext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b470b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting into vectors using openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cff7e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings=OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d37b199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "vectorstoredb=FAISS.from_documents(documents,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d48d62b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x2559947f2b0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstoredb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c30ffbc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'promptDynamic system promptInvocationAdvanced conceptsStructured outputToolStrategyProviderStrategyMemoryDefining state via middlewareDefining state via state_schemaStreamingMiddlewareCore componentsAgentsCopy pageCopy pageAgents combine language models with tools to create systems that can reason about tasks, decide which tools to use, and iteratively work towards solutions.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#QUERY FROM A VECTOR DB\n",
    "query=\"what is provider strategy\"\n",
    "result=vectorstoredb.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8ad8408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm=ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd959660",
   "metadata": {},
   "source": [
    "Custom prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7a3e049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Answer the following question based only on the provided context.\n",
    "\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b08e28c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\n    Answer the following question based only on the provided context.\\n\\n    <context>\\n    {context}\\n    </context>\\n\\n    '), additional_kwargs={})])\n",
       "| ChatOpenAI(profile={'max_input_tokens': 128000, 'max_output_tokens': 16384, 'image_inputs': True, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True, 'structured_output': True, 'image_url_inputs': True, 'pdf_inputs': True, 'pdf_tool_message': True, 'image_tool_message': True, 'tool_choice': True}, client=<openai.resources.chat.completions.completions.Completions object at 0x000002559947F5B0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000025599678FD0>, root_client=<openai.OpenAI object at 0x000002559947F490>, root_async_client=<openai.AsyncOpenAI object at 0x000002559947F7C0>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f739e32e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The provided context explains that agents use language models in conjunction with tools to build systems capable of reasoning about tasks, selecting appropriate tools, and progressively working toward solutions.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "document_chain.invoke({\n",
    "    \"input\":\"create_agent provides a production-ready agent implementation.\",\n",
    "    \"context\":[Document(page_content=\"Agents combine language models with tools to create systems that can reason about tasks, decide which tools to use, and iteratively work towards solutions.\")]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16527718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x2559947f2b0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Input --->Retreiver(Interface)-->VectorStoreDb\n",
    "vectorstoredb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99df02b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=vectorstoredb.as_retriever() # from vectorstoredb\n",
    "\n",
    "from langchain_classic.chains import create_retrieval_chain\n",
    "retrieval_chain = create_retrieval_chain(retriever,document_chain) #document_chain gives context info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b245467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000002559947F2B0>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\n    Answer the following question based only on the provided context.\\n\\n    <context>\\n    {context}\\n    </context>\\n\\n    '), additional_kwargs={})])\n",
       "            | ChatOpenAI(profile={'max_input_tokens': 128000, 'max_output_tokens': 16384, 'image_inputs': True, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True, 'structured_output': True, 'image_url_inputs': True, 'pdf_inputs': True, 'pdf_tool_message': True, 'image_tool_message': True, 'tool_choice': True}, client=<openai.resources.chat.completions.completions.Completions object at 0x000002559947F5B0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000025599678FD0>, root_client=<openai.OpenAI object at 0x000002559947F490>, root_async_client=<openai.AsyncOpenAI object at 0x000002559947F7C0>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c178326a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get the response from the llm \n",
    "response=retrieval_chain.invoke({\"input\":\"create_agent provides a production-ready agent implementation.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0322ce0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'create_agent provides a production-ready agent implementation.',\n",
       " 'context': [Document(id='16098585-0723-444f-bb5a-30abc66d7e8b', metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='create_agent provides a production-ready agent implementation.\\nAn LLM Agent runs tools in a loop to achieve a goal.\\nAn agent runs until a stop condition is met - i.e., when the model emits a final output or an iteration limit is reached.'),\n",
       "  Document(id='985dd908-7ff6-4a14-9805-650961f5f699', metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='create_agent builds a graph-based agent runtime using LangGraph. A graph consists of nodes (steps) and edges (connections) that define how your agent processes information. The agent moves through this graph, executing nodes like the model node (which calls the model), the tools node (which executes tools), or middleware.Learn more about the Graph API.\\n\\u200bCore components\\n\\u200bModel\\nThe model is the reasoning engine of your agent. It can be specified in multiple ways, supporting both static and dynamic model selection.\\n\\u200bStatic model\\nStatic models are configured once when creating the agent and remain unchanged throughout execution. This is the most common and straightforward approach.\\nTo initialize a static model from a model identifier string:\\nCopyfrom langchain.agents import create_agent\\n\\nagent = create_agent(\"openai:gpt-5\", tools=tools)'),\n",
       "  Document(id='cc1b51f9-4c44-47b1-b7ee-b86b8d774692', metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='agent = create_agent(\\n    model=\"gpt-4o\",\\n    response_format=ProviderStrategy(ContactInfo)\\n)\\n\\nAs of langchain 1.0, simply passing a schema (e.g., response_format=ContactInfo) will default to ProviderStrategy if the model supports native structured output. It will fall back to ToolStrategy otherwise.\\nTo learn about structured output, see Structured output.\\n\\u200bMemory\\nAgents maintain conversation history automatically through the message state. You can also configure the agent to use a custom state schema to remember additional information during the conversation.\\nInformation stored in the state can be thought of as the short-term memory of the agent:\\nCustom state schemas must extend AgentState as a TypedDict.\\nThere are two ways to define custom state:\\n\\nVia middleware (preferred)\\nVia state_schema on create_agent'),\n",
       "  Document(id='a5857bae-b3e2-46b9-b49a-c497928a51cf', metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='Agents - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationCore componentsAgentsLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartChangelogPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingStructured outputMiddlewareOverviewBuilt-in middlewareCustom middlewareAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryAgent developmentLangSmith StudioTestAgent Chat UIDeploy with LangSmithDeploymentObservabilityOn this pageCore componentsModelStatic modelDynamic modelToolsDefining toolsTool error handlingTool use in the ReAct loopDynamic toolsSystem promptDynamic system promptInvocationAdvanced conceptsStructured outputToolStrategyProviderStrategyMemoryDefining state via middlewareDefining state via state_schemaStreamingMiddlewareCore')],\n",
       " 'answer': 'How does an agent in the described context maintain conversation history, and is there a way to customize the state schema for remembering additional information during a conversation?\\n\\nAn agent maintains conversation history automatically through the message state, which acts as the short-term memory of the agent. You can customize the state schema to remember additional information by extending `AgentState` as a `TypedDict`. There are two methods to define a custom state: via middleware (which is the preferred method) or via `state_schema` when creating the agent.'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dde52f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='16098585-0723-444f-bb5a-30abc66d7e8b', metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='create_agent provides a production-ready agent implementation.\\nAn LLM Agent runs tools in a loop to achieve a goal.\\nAn agent runs until a stop condition is met - i.e., when the model emits a final output or an iteration limit is reached.'),\n",
       " Document(id='985dd908-7ff6-4a14-9805-650961f5f699', metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='create_agent builds a graph-based agent runtime using LangGraph. A graph consists of nodes (steps) and edges (connections) that define how your agent processes information. The agent moves through this graph, executing nodes like the model node (which calls the model), the tools node (which executes tools), or middleware.Learn more about the Graph API.\\n\\u200bCore components\\n\\u200bModel\\nThe model is the reasoning engine of your agent. It can be specified in multiple ways, supporting both static and dynamic model selection.\\n\\u200bStatic model\\nStatic models are configured once when creating the agent and remain unchanged throughout execution. This is the most common and straightforward approach.\\nTo initialize a static model from a model identifier string:\\nCopyfrom langchain.agents import create_agent\\n\\nagent = create_agent(\"openai:gpt-5\", tools=tools)'),\n",
       " Document(id='cc1b51f9-4c44-47b1-b7ee-b86b8d774692', metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='agent = create_agent(\\n    model=\"gpt-4o\",\\n    response_format=ProviderStrategy(ContactInfo)\\n)\\n\\nAs of langchain 1.0, simply passing a schema (e.g., response_format=ContactInfo) will default to ProviderStrategy if the model supports native structured output. It will fall back to ToolStrategy otherwise.\\nTo learn about structured output, see Structured output.\\n\\u200bMemory\\nAgents maintain conversation history automatically through the message state. You can also configure the agent to use a custom state schema to remember additional information during the conversation.\\nInformation stored in the state can be thought of as the short-term memory of the agent:\\nCustom state schemas must extend AgentState as a TypedDict.\\nThere are two ways to define custom state:\\n\\nVia middleware (preferred)\\nVia state_schema on create_agent'),\n",
       " Document(id='a5857bae-b3e2-46b9-b49a-c497928a51cf', metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='Agents - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationCore componentsAgentsLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartChangelogPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingStructured outputMiddlewareOverviewBuilt-in middlewareCustom middlewareAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryAgent developmentLangSmith StudioTestAgent Chat UIDeploy with LangSmithDeploymentObservabilityOn this pageCore componentsModelStatic modelDynamic modelToolsDefining toolsTool error handlingTool use in the ReAct loopDynamic toolsSystem promptDynamic system promptInvocationAdvanced conceptsStructured outputToolStrategyProviderStrategyMemoryDefining state via middlewareDefining state via state_schemaStreamingMiddlewareCore')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a81c03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
